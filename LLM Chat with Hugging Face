# Updated package installations for better performance and compatibility:
!pip install langchain==0.3.26
!pip install langchain-openai==0.3.28
!pip install langchain-community==0.3.19
!pip install huggingface_hub==0.27.1


!pip install transformers==4.50.0
!pip install langchain_huggingface


from getpass import getpass

OPENAI_KEY = getpass('Enter Open AI API Key: ')


import os

os.environ['OPENAI_API_KEY'] = OPENAI_KEY


from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = getpass('Please enter your HuggingFace Token here: ')


import os

os.environ['HF_TOKEN'] = HUGGINGFACEHUB_API_TOKEN
os.environ['OPENAI_API_KEY'] = OPENAI_KEY


import os
from langchain_openai import ChatOpenAI

# Set your API Key
os.environ["OPENAI_API_KEY"] = OPENAI_KEY

# Use ChatOpenAI for gpt-4o-mini
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)



prompt = """Explain what is Generative AI in 3 bullet points"""
print(prompt)



response = chatgpt.invoke(prompt)
print(response.content)



!pip install -U langchain-google-genai



# 1. Install the package (only needs to be done once per session)
# !pip install -U langchain-google-genai

# 2. Import the library
from langchain_google_genai import ChatGoogleGenerativeAI
import getpass
import os

# 3. Set up your API Key 
# (It's safer to use getpass so your key isn't visible in the code)
if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google API Key:AIzaSyBOkhLuh4Tn5MasFJiqND-umW0DC8zoEQ8")

# 4. Initialize and Invoke
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
response = llm.invoke("Hello, are you working now?")

print(response.content)




response = chatgpt.invoke(prompt)
print(response.content)




from langchain_google_genai import ChatGoogleGenerativeAI
import os

# 1. Define the model (Make sure your API Key is set here)
# Replace 'YOUR_KEY_HERE' with your actual API key
chatgpt = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key="AIzaSyBOkhLuh4Tn5MasFJiqND-umW0DC8zoEQ8"
)

prompt = "Hello! Are you configured correctly?"

# 3. NOW you can call it
response = chatgpt.invoke(prompt)

# 4. Print the result
print(response.content)



import os
from langchain_openai import ChatOpenAI


# Use ChatOpenAI for gpt-4o-mini
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)



prompt = """Explain what is Generative AI in 3 bullet points"""
print(prompt)



response = chatgpt.invoke(prompt)
response



print(response.content)



##Accessing Open Source LLMs with HuggingFace and LangChain

##LLaMA

from langchain_huggingface import HuggingFaceEndpoint
base_url="https://router.huggingface.co/v1"
from huggingface_hub import InferenceClient
import os
os.environ['HF_TOKEN'] = 'Hugging Token'             # Replace with your actual token
client = InferenceClient(api_key=os.environ["HF_TOKEN"])

# Automatic provider selection (default: "auto")
result = client.chat_completion(
    model="meta-llama/Llama-3.1-8B-Instruct",
    # Defaults to "auto" selection of the provider
    # provider="auto",
    messages=[{"role": "user", "content": "Hello!"}],
)

result

print(result.choices[0].message.content)

# LLaMA 3 model repo
repo_id = "meta-llama/Llama-3.1-8B-Instruct"

# Parameters for inference
llama3_params = {
    "do_sample": False,          # greedy decoding (temperature = 0)
    "return_full_text": False,   # don't return input prompt
    "max_new_tokens": 1000,      # maximum tokens for the response
}

# LLaMA3 model using LangChain endpoint
llm = HuggingFaceEndpoint(
    repo_id=repo_id,
    temperature=0.5,
    task="conversational",
    **llama3_params
)

# Inference client for HuggingFace Hub
client = InferenceClient(token=os.environ["HF_TOKEN"])


# Prompt for LLaMA3 (system + user + assistant format)
formatted_prompt = (
    "<|system|>You are a helpful assistant.<|end|>\n"
    "<|user|>Explain what is Generative AI in 3 bullet points<|end|>\n"
    "<|assistant|>"
)
formatted_prompt


# Get response from the model using chat completion
result = client.chat_completion(
    model=repo_id,
    messages=[{"role": "user", "content": formatted_prompt}],
    max_tokens=1000,
    temperature=0.5
)
print(result.choices[0].message.content)


response = client.chat_completion(
    model=repo_id,
    messages=[{"role": "user", "content": formatted_prompt}],
    max_tokens=1000,
    temperature=0.5
)
print(response.choices[0].message.content)


formatted_prompt = (
    "<|system|>You are a helpful assistant.<|end|>\n"
    "<|user|>Explain the concepts of Gravitational waves and how they effect other cellistial bodeis<|end|>\n"
    "<|assistant|>"
)
response = client.chat_completion(
    model=repo_id,
    messages=[{"role": "user", "content": formatted_prompt}],
    max_tokens=1000,
    temperature=0.5
)
print(response.choices[0].message.content)

response


##Accessing Google Gemma 2B Instruct

from langchain_huggingface import HuggingFacePipeline
import os

prompt


# New Method
response = client.chat_completion(
    model=gemma_repo_id,
    messages=[{"role": "user", "content": prompt}],
    max_tokens=1000,
    temperature=0.5,
)
print(response.choices[0].message.content)

##Accessing Google Gemma 2B and running it locally

from langchain_huggingface import HuggingFacePipeline

!pip install hf-xet

gemma_params = {
                  "do_sample": False, # greedy decoding - temperature = 0
                  "return_full_text": False, # don't return input prompt
                  "max_new_tokens": 1000, # max tokens answer can go upto
                }

local_llm = HuggingFacePipeline.from_model_id(
    model_id="google/gemma-1.1-2b-it",
    task="text-generation",
    pipeline_kwargs=gemma_params,
    # device=0 # when running on Colab selects the GPU, you can change this if you run it on your own instance if needed
)

local_llm


